{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Working with different kinds of ‘text’ in the Digital Humanities_\n",
    "## Singapore University of Technology and Design, 18-19 March, 2019\n",
    "## Introduction to NLP: From Counting to Lanugage Models\n",
    "\n",
    "Welcome to From Counting to Lanugage Models!\n",
    "This is a hands-on workshop focusing on various foundation techniques for Natural Language Processing and their applications in Digital Humanities nad beyond. It anything, it's a methods workshop more than a critical or theoretical workshop: the emphasis is put on the how rathen than the why or what for.\n",
    "\n",
    "<!--\n",
    "The workshop will be split into 4 sections with 10 minute breaks in-between. The sections get incrementally more advanced, building on concepts and methods from the previous sections.\n",
    "-->\n",
    "\n",
    "\n",
    "To follow along, you can run the script portions piecemeal, in order, as we progress through the workshop material. Up to you. Familiarity with programming concepts and Python is required; Numpy and Jupyter desirable.\n",
    "\n",
    "Instructor:\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://postdata.linhd.uned.es/wp-content/uploads/2019/02/javierweb.jpg\"\n",
    "         alt=\"Javier's picture\">\n",
    "    <figcaption>\n",
    "        <div align=\"center\">\n",
    "        <strong>Javier de la Rosa</strong>\n",
    "        <br/>\n",
    "        <em>versae@linhd.uned.es</em>, <em><a href=\"https://twitter.com/versae\">@versae</a></em>\n",
    "        <br/>\n",
    "        NLP Postdoctoral Fellow at <a href=\"http://postdata.linhd.uned.es/\">UNED's POSTADA Project</a>\n",
    "       </div>\n",
    "    </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we covering today?\n",
    "- What is NLP\n",
    "- NLP in Python\n",
    "- Tokenization\n",
    "- Part of Speech Tagging\n",
    "- Named Entity Recognition and Relation Detection\n",
    "- Word transformations\n",
    "- Keywords in context\n",
    "- Counting\n",
    "- TF-IDF and Document-Term Matrices\n",
    "- Topic Models\n",
    "- Clustering and PCA\n",
    "- ~~Word-word matrices~~\n",
    "- ~~Word embeddings~~\n",
    "- ~~Language models~~\n",
    "\n",
    "Use cases:\n",
    "- Readability indices\n",
    "- Corpus level statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP in Python\n",
    "\n",
    "Python is builtin with a very mature regular expression library, which is the building block of natural language processing. However, more advanced tasks need different libraries. Traditionally, in the Python ecosystem the Natural Language Processing Toolkit, abbreviated as `NLTK`, has been until recently the only working choice. Unfortunately, the library has not aged well, and even though it's updated to work with the newer versions of Python, it does not provide us the speed we might need to process large corpora, as its intended use is merely educational.\n",
    "\n",
    "Another solution that appeared recently is called `spaCy`, and it is much faster since is written in a pseudo-C Python language optimized for speed called Cython. See the [documentation](https://spacy.io/usage/models) for details.\n",
    "\n",
    "Both these libraries are complex and therefore there exist wrappers around them to simplify their APIs. The two more popular are `Textblob` for NLTK and CLiPS Parser, and `textacy` for spaCy.  In this workshop we will be using spaCy with a touch of textacy thrown in at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "import sys\n",
    "!pip install Cython\n",
    "!pip install spacy nltk textacy textblob requests matplotlib scikit-learn\n",
    "!python -m spacy download en\n",
    "!python -m spacy download es\n",
    "!python -m nltk.downloader all\n",
    "print(\"All done!\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the English data for now. Support for other [languages is available as well](https://spacy.io/usage/models), although some features might not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to need a couple of helper functions to retrieve some texts from US presidents' State of the Union speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import requests\n",
    "\n",
    "def get_text(url):\n",
    "    return requests.get(url).text\n",
    "\n",
    "def get_speech(url):\n",
    "    page = get_text(url)\n",
    "    full_text = page.split('\\n')\n",
    "    return \" \".join(full_text[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/clinton2000.txt\"\n",
    "clinton_speech = get_speech(clinton_url)\n",
    "print(clinton_speech[:500],  \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a SpaCy `Document` of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(clinton_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While basic, some cleaning has been done already. Compare these 2 texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(clinton_url)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speech[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, the act of splitting text is called tokenization, and each of the individual chunks is called a token. Therefore, we can talk about word tokenization or sentence tokenization depending on what it is that we need to divide the text into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level\n",
    "for token in doc[:20]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence level\n",
    "for token in [sent for sent in doc.sents][:10]:\n",
    "    print(\"- \", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very easily, SpaCy allows for the extraction of noun phrases, which can be useful sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# noun phrases\n",
    "for phrase in list(doc.noun_chunks)[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "SpaCy also allows you to perform Part-Of-Speech tagging, a kind of grammatical chunking, out of the box. For POS, SpaCy follows the Universal Dependencies tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple part of speech tag\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.pos_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed information can also be obtained if available. In these cases, the format will depend on the language and corpus used. For English, [MBSP tags](http://www.clips.ua.ac.be/pages/mbsp-tags) are used, while in Spanish, the [Universal Feature inventory](https://universaldependencies.org/u/feat/index.html) is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed tag\n",
    "# For what these tags mean, you might check out http://www.clips.ua.ac.be/pages/mbsp-tags\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.tag_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A syntactic dependency is a relation between two words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic dependency\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.dep_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's easier to understand with a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the sentence\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sent = list(doc.sents)[0]\n",
    "first_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "single_doc = nlp(str(first_sent))\n",
    "options = {\"compact\": True, 'bg': 'white',\n",
    "           'color': 'black', 'font': 'Source Sans Pro'}\n",
    "displacy.render(single_doc, style=\"dep\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "Write a function `count_chars(text)` that receives `text` and returns the total number of characters ignoring spaces and punctuation marks. For example, `count_chars(\"Well, I am not 90 years old.\")` should return `20`.\n",
    "<br/>\n",
    "* **Hint**: You could count the characters in the words.*\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    doc = ...\n",
    "    words = [... for token in doc if ... != 'PUNCT']\n",
    "    return ...\n",
    "\n",
    "count_chars(\"Well, I am not 30 years old.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.\n",
    "\n",
    "For English, SpaCy uses the [OntoNotes 5](https://catalog.ldc.upenn.edu/LDC2013T19) corpus, which is sufficiently rich and specific regarding the [information it can caputre](https://spacy.io/api/annotation#named-entities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents[:20]:\n",
    "    print(ent.text, ent.label_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working on tokens, you can still access entity type. Notice, though that the phrase entities are broken up here because we're iterating over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:150]:\n",
    "    if token.ent_type_ is not '':\n",
    "        print(token.text, token.ent_type_, f\"({spacy.explain(token.ent_type_)})\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy comes with built in entity visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(single_doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "for sent in list(doc.sents)[:10]:\n",
    "    displacy.render(nlp(sent.text), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to train your own entity recognition model, and to train other types of models in SpaCy, but you need sufficient labeled data to make it work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:20]:\n",
    "    print(token.text, token.lemma_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp('here are octopi'):\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp('There have been many mice and geese surrounding the pond.'):\n",
    "    print(token, token.lemma_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we just want to lematize verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:1500]:\n",
    "    if token.tag_ == \"VBP\":\n",
    "        print(token.text, token.lemma_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using the simple part of speech instead of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:250]:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        print(token.text, token.lemma_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:20]:\n",
    "    print(token.text, token.lower_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword in Context (KWIC)\n",
    "\n",
    "\"A KWIC index, [the most common format for concordance lines], is formed by sorting and aligning the words within an article title to allow each word (except the stop words) in titles to be searchable alphabetically in the index.\" -- https://en.wikipedia.org/wiki/Key_Word_in_Context.\n",
    "\n",
    "It also allows for a quick exploration of how specific words are being used and in what context. One quick (but potentially very resource intensive) way of computing KWIC is by using n-grams. N-grams are sliced splits of tokens in groups of _n_, thus a 2-gram (bi-gram) is a group of 2 words, a 3-gram a group of 3. The way they are built is a follows.\n",
    "\n",
    "```\n",
    "This is a sentence\n",
    "```\n",
    "\n",
    "If we extract all bi-grams, we get\n",
    "\n",
    "`This, is`, `is a`, `a sentence`.\n",
    "\n",
    "And if we now focus in, for example, the context of `a`, we can see very quickly that is being used as follows:\n",
    "```\n",
    "is a\n",
    "   a sentence.\n",
    "```\n",
    "\n",
    "SpaCy does not support natively splitting by n-grams, but its wrapper `textacy` does, so all we need to do is to reconstruct a basic search over the ngrams with textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textacy.extract.ngrams(nlp(\"This is a sentence\"), 2, filter_stops=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, textacy already includes KWIC by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.text_utils.KWIC(doc.text, \"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting\n",
    "\n",
    "Counting is at the basics of Natural Language Processing, and in some sub-disciplines is still the king of methods. Let's see a couple of approaches to counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the builtin `Counter()` class and a sample document containing a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sents = \"One fish, two fish, red fish, blue fish. One is less than two.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of the words without the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = nlp(sample_sents)\n",
    "words = [token.text for token in new_doc if token.pos_ is not 'PUNCT']\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To all the distinct words in a document or a corpus, we call vocabulary or lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the frequency of each term in a document can be then determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter[\"fish\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basics of what is known as bag of words (BoW), which is widely used technice to transform text into numbers (thus: vectorization) suitable for machine learning algorithms. It's also supported in textacy out of the box (with some caveats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc = textacy.Doc(nlp(sample_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc.to_bag_of_words(normalize=None, as_strings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that textacy always removes stop words. It should actually be optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc.count(\"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "Let's define the lexicon of a person as the number of different words she uses to speak. Write a function `get_lexicon(text, n)` that receives `text` and `n` and returns the lemmas of nouns, verbs, and adjectives that are used at least `n` times.\n",
    "<br/>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon(text, n):\n",
    "    doc = nlp(text)\n",
    "    # return a list of words that     \n",
    "    words = [... for token in doc if token.pos_ in ...]\n",
    "    # count the words     \n",
    "    counter = Counter(...)\n",
    "    # filter by number\n",
    "    filtered_words = [word for word in counter if ...]\n",
    "    return sorted(filtered_words)\n",
    "    \n",
    "get_lexicon(clinton_speech, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Document-Term Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our intuition, we think that the words which appear more often should have a greater weight in textual data analysis, but that's not always the case. Words such as “the”, “will”, and “you” —stopwords— appear the most in a corpus of text, but are of very little significance. Instead, the words which are rare are the ones that actually help in distinguishing between the data, and carry more weight.\n",
    "\n",
    "TF-IDF stands for “Term Frequency — Inverse Data Frequency”, and it's just a vectorization algorithm that tries to assign weights based on the relative importance of a word within a document and the corpus it belongs to.\n",
    "\n",
    "- Term Frequency (tf): gives the frequency of the word ($t$) in each document ($d$) in the corpus ($D$). It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "- Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score.\n",
    "\n",
    "$$idf( t, D ) = log \\frac{ \\text{| } D \\text{ |} }{ 1 + \\text{| } \\{ d \\in D : t \\in d \\} \\text{ |} }$$\n",
    "\n",
    "Combining these two we come up with the TF-IDF score for a word in a document in the corpus. It is the product of tf and idf.\n",
    "\n",
    "$$tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )$$\n",
    "\n",
    "Let's now compile a tiny corpus to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright today.\",\n",
    "    \"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus('en', texts=raw_corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm.vectorizers import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth', apply_dl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform([\n",
    "    doc.to_terms_list(normalize=None, as_strings=True, ngrams=(1,), filter_stops=False)\n",
    "    for doc in corpus.docs\n",
    "]).todense().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix above is the document-term matrix, in which (although now transposed), rows represent documents and columns weights, in this case tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could also be obtained a doc-term matrix with raw counts instead (tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False)\n",
    "vectorizer.fit_transform([\n",
    "    doc.to_terms_list(normalize=None, as_strings=True, ngrams=(1,), filter_stops=False)\n",
    "    for doc in corpus.docs\n",
    "]).todense().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try with a bigger corpus of US President's speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/clinton2000.txt\"\n",
    "bush_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/bush2008.txt\"\n",
    "obama_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/obama2016.txt\"\n",
    "trump_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/trump.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speech = get_speech(clinton_url)\n",
    "bush_speech = get_speech(bush_url)\n",
    "obama_speech = get_speech(obama_url)\n",
    "trump_speech = get_speech(trump_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = textacy.Corpus(\n",
    "    'en',\n",
    "    texts=[clinton_speech, bush_speech, obama_speech, trump_speech],\n",
    "    metadatas=[{\"name\": \"clinton\"}, {\"name\": \"bush\"}, {\"name\": \"obama\"}, {\"name\": \"trump\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(tf_type='linear', apply_idf=True, idf_type='smooth', apply_dl=False)  # tf-idf\n",
    "terms_list = [\n",
    "    doc.to_terms_list(normalize=None, as_strings=True, ngrams=(1,), filter_stops=True)\n",
    "    for doc in speeches.docs\n",
    "]\n",
    "doc_term_matrix = vectorizer.fit_transform(terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.terms_list[250:275]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "Once we have our weighted document-term matrix, is easy to calculate what are the more prominent topics using topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('lsa', n_topics=20)\n",
    "model.fit(doc_term_matrix)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=range(4)):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.termite_plot(doc_term_matrix, vectorizer.id_to_term,\n",
    "                   topics=range(4),  n_terms=25, sort_terms_by='seriation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now also possible to cluster the documents based in their tf-idf weihts using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc_term_matrix.todense()\n",
    "labels_color_map = {\n",
    "    'Clinton': '#20b2aa', 'Bush': '#ff7373', 'Obama': '#005073', 'Trump': '#F0926E'\n",
    "}\n",
    "labels = list(labels_color_map.keys())\n",
    "reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "ax.legend(labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability indices\n",
    "\n",
    "Readability indices are ways of assessing how easy or complex it is to read a particular text based on the words and sentences it has. They usually output scores that correlate with grade levels.\n",
    "\n",
    "A couple of indices that are presumably easy to calculate are the [Auto Readability Index (ARI)](https://en.wikipedia.org/wiki/Automated_readability_index) and the [Coleman-Liau Index](https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index):\n",
    "\n",
    "$$\n",
    "ARI = 4.71\\frac{chars}{words}+0.5\\frac{words}{sentences}-21.43\n",
    "$$\n",
    "$$ CL = 0.0588\\frac{letters}{100 words} - 0.296\\frac{sentences}{100words} - 15.8 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: the tokens in spacy include punctuation. to get this right, we should remove punct\n",
    "# we then have to make sure our functions handle lists of words rather than spacy doc objects\n",
    "\n",
    "def coleman_liau_index(doc, words):\n",
    "    return (0.0588 * letters_per_100(doc)) - (0.296 * sentences_per_100(doc, words)) - 15.8\n",
    "\n",
    "def count_chars(words):\n",
    "    return sum(len(w) for w in words)\n",
    "\n",
    "def sentences_per_100(doc, words):\n",
    "    return (len(list(doc.sents)) / len(words)) * 100\n",
    "\n",
    "def letters_per_100(words):\n",
    "    return (count_chars(words) / len(words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get just the words, without punctuation tokens\n",
    "def return_words(doc):\n",
    "    return [token.text for token in doc if token.pos_ is not 'PUNCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fancy_doc = nlp(\"Regional ontology, clearly defined by Heidegger, equals, if not surpasses, the earlier work of Heidegger's own mentor, Husserl\")\n",
    "fancy_words = return_words(fancy_doc)\n",
    "fancy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coleman_liau_index(fancy_doc, fancy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(clinton_speech)\n",
    "clinton_speech_words = return_words(doc)\n",
    "coleman_liau_index(doc, clinton_speech_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "Write a function `auto_readability_index(doc)` that receives a spacy `Doc` and returns the Auto Readability Index (ARI) score as defined above. \n",
    "<br/>\n",
    "* **Hint**: Feel free to use functions we've defined before.*\n",
    "   \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_readability_index(doc):\n",
    "    words = ...\n",
    "    chars = ...\n",
    "    words = ...\n",
    "    sentences = ...\n",
    "    return (4.71 * (chars / words)) + (0.5 * (words / sentences)) - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_readability_index(fancy_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_readability_index(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/clinton2000.txt\"\n",
    "bush_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/bush2008.txt\"\n",
    "obama_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/obama2016.txt\"\n",
    "trump_url = \"https://raw.githubusercontent.com/sul-cidr/python_workshops/master/data/trump.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speech = get_speech(clinton_url)\n",
    "bush_speech = get_speech(bush_url)\n",
    "obama_speech = get_speech(obama_url)\n",
    "trump_speech = get_speech(trump_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {\n",
    "    \"clinton\": nlp(clinton_speech),\n",
    "    \"bush\": nlp(bush_speech),\n",
    "    \"obama\": nlp(obama_speech),\n",
    "    \"trump\": nlp(trump_speech),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name\", \"Chars\", \"Words\", \"Unique\", \"Sentences\", sep=\"\\t\")\n",
    "for speaker, speech in speeches.items():\n",
    "    words = return_words(speech)\n",
    "    print(speaker, count_chars(words), len(words), len(set(words)), len(list(speech.sents)), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "Write a function `avg_sentence_length(blob)` that receives a spaCy `doc` and returns the average number of words in a sentence for the doc. You might need to use our `return_words` function.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average sentence length\n",
    "def avg_sentence_length(doc):\n",
    "    return ... / len(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker, speech in speeches.items():\n",
    "    print(speaker, avg_sentence_length(speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might stop to ask why Obama's speech seems to have shorter sentences. Is it deliberate rhetorical choice? Or could it be an issue with the data itself?\n",
    "\n",
    "In this case, if we look closely at the txt file, we can see that the transcription of the speech included the world 'applause' as a one word sentence throughout the text. Let's see what happens if we filter that out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_clean_speech = obama_speech.replace(\"(Applause.)\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare lengths of the texts. We should see a difference.\n",
    "\n",
    "len(obama_speech), len(obama_clean_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's recheck the average sentence length of Obama's speech.\n",
    "avg_sentence_length(nlp(obama_clean_speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {\n",
    "    \"clinton\": nlp(clinton_speech),\n",
    "    \"bush\": nlp(bush_speech),\n",
    "    \"obama\": nlp(obama_clean_speech),\n",
    "    \"trump\": nlp(trump_speech),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a quick function to get the most common words used by each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(doc, n):\n",
    "    words = return_words(doc)\n",
    "    c = Counter(words)\n",
    "    return c.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for speaker, speech in speeches.items():\n",
    "    print(speaker, most_common_words(speech, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see quickly that we need to remove some of these most common words. To do this, we'll use common lists of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(list(STOP_WORDS)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure we've got all the punctuation out and to remove some contractions, we'll have a custom stoplist\n",
    "custom_stopwords = [',', '-', '.', '’s', '-', ' ', '(', ')', '--', '---', 'n’t', ';', \"'s\", \"'ve\", \"  \", \"’ve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(doc, n):\n",
    "    words = [token.text for token in doc if token.pos_ is not 'PUNCT' \n",
    "             and token.lower_ not in STOP_WORDS and token.text not in custom_stopwords]\n",
    "    c = Counter(words)\n",
    "    return c.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker, speech in speeches.items():\n",
    "    print(speaker, \": \", most_common_words(speech, 10), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of exploratory work is often the first step in figuring out how to clean a text for text analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess the lexical richness, defined as the ratio of number of unique words by the number of total words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_richness(doc):\n",
    "    words = return_words(doc)\n",
    "    return len(set(words)) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker, speech in speeches.items():\n",
    "    print(speaker, lexical_richness(speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the readbility scores for all four speeches now\n",
    "\n",
    "For the Automated Readability Index, you can get the appropriate grade level here: https://en.wikipedia.org/wiki/Automated_readability_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for speaker, speech in speeches.items():\n",
    "    words = return_words(speech)\n",
    "    print(speaker, \"ARI:\", auto_readability_index(speech), \"CL:\", coleman_liau_index(speech, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some comparison, let's also look at some stats calculated through Textacy. We'll see the ARI and CL scores, which use the same formulas we used. However, you might notice that the scores are different. To understand why, you have to dig into the source code for Textacy, where you'll find that it filters out punctuation in creating the word list, which affects the number of characters. It also lowercases the punctuation-filtered words before creating the set of unique words, decreasing that number as well compared to how we calculated it here. These changes affect both the ARI and CL scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "# https://en.wikipedia.org/wiki/Automated_readability_index\n",
    "txt_speeches = [clinton_speech, bush_speech, obama_clean_speech, trump_speech]\n",
    "corpus = textacy.Corpus('en', txt_speeches)\n",
    "for doc in corpus:\n",
    "    stats = textacy.text_stats.TextStats(doc)\n",
    "    print({\n",
    "        \"ARI\": stats.automated_readability_index,\n",
    "        \"CL\": stats.coleman_liau_index,\n",
    "        \"stats\": stats.basic_counts\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have such a significant difference in the CL scores? Let's look quickly at the textacy implementation: https://github.com/chartbeat-labs/textacy/blob/5927d539dd989c090f8a0b0c06ba40bb204fce82/textacy/text_stats.py#L277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name\", \"Chars\", \"Words\", \"Unique\", \"Sentences\", sep=\"\\t\")\n",
    "for speaker, speech in speeches.items():\n",
    "    words = return_words(speech)\n",
    "    print(speaker, count_chars(words), len(words), len(set(words)), len(list(speech.sents)), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinton, bush, obama, trump\n",
    "for doc in corpus:\n",
    "    stats = textacy.text_stats.TextStats(doc)\n",
    "    print({\n",
    "        \"stats\": stats.basic_counts\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
